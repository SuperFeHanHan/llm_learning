# RLHF

---

èµ„æ–™ï¼š

- [https://datawhalechina.github.io/easy-rl/#/](https://datawhalechina.github.io/easy-rl/#/)

# æ—¶é—´çº¿

- **2017**: Proximal Policy Optimization Algorithms
- **2023**: Direct Preference Optimization: Your Language Model is Secretly a Reward Model
- **2024**: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

# åŸºæœ¬æ¦‚å¿µ

| **æ¦‚å¿µ** | **è‹±æ–‡** | **ç¬¦å·** | **è§£é‡Š** |
| --- | --- | --- | --- |
| æ™ºèƒ½ä½“ | Agent |  |  |
| ç¯å¢ƒ | Environment |  |  |
| çŠ¶æ€ | State | $s_t$ |  |
| çŠ¶æ€ç©ºé—´ |  | $S$ | æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€é›†åˆ |
| åŠ¨ä½œ | Action | $a_t$ | æ™ºèƒ½ä½“åœ¨æ¯ä¸ªçŠ¶æ€$s_t$ä¸‹å¯ä»¥é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ$a_t$ï¼Œä»¥æœŸæœ›è·å¾—æœ€å¤§çš„é•¿æœŸå¥–åŠ±ã€‚ |
| åŠ¨ä½œç©ºé—´ |  | $A$ | æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œé›†åˆ |
| å¥–åŠ± | Reward | $r_t=R(s_t,a_t,s_{t+1})$
$R(s, a)=\mathbb{E}\left[r_{t+1} \mid s_t=s, a_t=a\right]$ | æ³¨æ„ï¼Œè¿™é‡Œä¸åŒæ¥æºçš„ä¸‹æ ‡å¯èƒ½æœ‰äº›å·®å¼‚ï¼Œæœ¬è´¨ä¸Šæ˜¯æƒ³è¯´$s_t,a_t$å¹¶æˆåŠŸåˆ°è¾¾$s_{t+1}$æ—¶è·å¾—çš„å¥–åŠ±ã€‚ |
| ç­–ç•¥ | Policy | $a_t = \pi_\theta(s_t)\\
a_t \sim \pi_\theta(\cdot|s_t)$ | åœ¨$s_t$ä¸‹æ™ºèƒ½ä½“åº”è¯¥é€‰å–ä¸‹ä¸€æ­¥åŠ¨ä½œçš„å‡†åˆ™ï¼Œåˆ†ä¸º**ç¡®å®šæ€§ç­–ç•¥(deterministic)**å’Œ**éšæœºæ€§ç­–ç•¥(stochastic)**ã€‚$\theta$æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚ |
| è¿åŠ¨è½¨è¿¹ | Trajectory | $\tau = (s_t, a_t, s_{t+1}, a_{t+1},....)$ |  |
|  | Transition | $p(s_{t+1}|s_t,a_t)$ | å› ä¸ºç¯å¢ƒå­˜åœ¨ä¸€å®šéšæœºæ€§ï¼Œåœ¨$s_t$ä¸‹é‡‡å–åŠ¨ä½œ$a_t$å¯èƒ½ä¼šåˆ°$s_{t+1}$ |
| ç´¯ç§¯å¥–åŠ± | Cumulative Reward | $R(\tau) = \sum_{t=0}^{T-1} r_t$ |  |
| æŠ˜æ‰£å¥–åŠ± | Discounted Cumulative Reward | $R(\tau) = \sum_{t=0}^{T-1} \gamma^t r_t$ | å› ä¸ºè¶Šè¿œçš„å¥–åŠ±è·Ÿå½“å‰åŠ¨ä½œçš„å…³ç³»è¶Šä½ |
| **å›æŠ¥** | Return | $G_t=\sum_{k=0}^\infty \gamma^k r_{t+k}$ |  |
| **çŠ¶æ€ä»·å€¼å‡½æ•°** | State-value function | $V_\pi(s)=E_{\pi}[G_t|S_t=s]$ | ä»æŸä¸ªçŠ¶æ€$s$å¼€å§‹ï¼ŒæŒ‰ç…§ç­–ç•¥$\pi$èƒ½å¾—åˆ°çš„ç´¯è®¡å¥–åŠ±çš„æœŸæœ›å€¼ã€‚ |
| **åŠ¨ä½œä»·å€¼å‡½æ•°** | Action-value function | $Q_\pi(s,a) = E_\pi[G_t|S_t=s, A_t=a]$ | ä»æŸä¸ªçŠ¶æ€$s$ä¸”å½“å‰æ‰§è¡ŒåŠ¨ä½œ$a$ï¼ŒæŒ‰ç…§ç­–ç•¥$\pi$èƒ½å¾—åˆ°çš„ç´¯è®¡å¥–åŠ±çš„æœŸæœ›å€¼ã€‚ |
| **ä¼˜åŠ¿å‡½æ•°** | Advantage Function | $A^\theta\left(s_t, a_t\right)$ | ä¸€èˆ¬ç”±Criticç½‘ç»œæ¥ä¼°è®¡ã€‚ |
| åŸºäºç­–ç•¥çš„æ–¹æ³• | Policy-based Method |  | Policy Gradient
æ”¯æŒè¿ç»­ç©ºé—´ï¼Œä½†é«˜æ–¹å·®ã€æ ·æœ¬æ•ˆç‡ä½ã€è®­ç»ƒä¸ç¨³å®š |
| åŸºäºä»·å€¼çš„æ–¹æ³• | Value-based Method |  | Q-learning, DQN
æ ·æœ¬æ•ˆç‡é«˜ã€è®­ç»ƒç¨³å®šï¼Œè¾ƒéš¾å¤„ç†è¿ç»­åŠ¨ä½œã€‚ |
| Actor-Critic | Actor-Critic |  | **Actor**: Policy-based Methodï¼Œå†³å®šæ™ºèƒ½ä½“å¦‚ä½•è¡ŒåŠ¨ï¼Œå³$\pi_\theta$
**Critic**: åˆ¤æ–­å½“å‰åŠ¨ä½œçš„å¥½åï¼Œå³$Q_\pi(s,a)$æˆ–$V_\pi(s)$ |
| åŒç­–ç•¥/åœ¨çº¿ | On-Policy |  | è¦å­¦ä¹ çš„æ™ºèƒ½ä½“å’Œä¸ç¯å¢ƒäº¤äº’çš„æ™ºèƒ½ä½“æ˜¯ç›¸åŒçš„ |
| å¼‚ç­–ç•¥/ç¦»çº¿ | Off-Policy |  | è¦å­¦ä¹ çš„æ™ºèƒ½ä½“å’Œä¸ç¯å¢ƒäº¤äº’çš„æ™ºèƒ½ä½“ä¸æ˜¯ç›¸åŒçš„ |
| æ—¶åºå·®åˆ† | Temporal Difference |  |  |

## Actor-Critic

### Actor (Policy Gradient, REINFORCE)

![$p_\theta$å³ä¸Šè¿°çš„$\pi_\theta$ï¼Œæ˜¯æˆ‘ä»¬è¦è®­ç»ƒçš„Policy Model
$R(\tau^i)$: ç¬¬$i$ä¸ªepisodeå¾—åˆ°çš„æ€»å¥–åŠ±
å®é™…å®ç°ä¸­ï¼Œå¯ä»¥ä½¿ç”¨pytorchçš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è¿™ä¸ªæŸå¤±ã€‚
$Loss = -\frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R\left(\tau^n\right) \log \pi_\theta\left(a_t^n \mid s_t^n\right)$](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image.png)

$p_\theta$å³ä¸Šè¿°çš„$\pi_\theta$ï¼Œæ˜¯æˆ‘ä»¬è¦è®­ç»ƒçš„Policy Model
$R(\tau^i)$: ç¬¬$i$ä¸ªepisodeå¾—åˆ°çš„æ€»å¥–åŠ±
å®é™…å®ç°ä¸­ï¼Œå¯ä»¥ä½¿ç”¨pytorchçš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è¿™ä¸ªæŸå¤±ã€‚
$Loss = -\frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R\left(\tau^n\right) \log \pi_\theta\left(a_t^n \mid s_t^n\right)$

- è¯æ˜ï¼šä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªæ¢¯åº¦ï¼ˆåŸºäºæœ€å¤§åŒ–Rewardçš„å‡è®¾ï¼‰
  
    ![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%201.png)
    

![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%202.png)

<aside>
ğŸ’¡

æ”¹è¿›æ–¹æ³•1ï¼š**æ·»åŠ åŸºçº¿$b$ï¼Œä¸€èˆ¬å–$b = E[R(\tau)]$ï¼Œå®è·µä¸­è®°å½•æ¯ä¸ªé‡‡æ ·å¾—åˆ°Rewardçš„å¹³å‡å³å¯ã€‚**

</aside>
$$
\boxed{\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}\left(R\left(\tau^n\right)-b\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)}
$$

- ç†ç”±ï¼šå› ä¸ºæˆ‘ä»¬æ˜¯é‡‡æ ·æ¥æ›´æ–°çš„ï¼Œå¦‚æœRewardè®¾ç½®éƒ½æ˜¯æ­£çš„ï¼ˆä¾‹å¦‚0-5åˆ†ï¼‰ï¼Œåˆ™è¢«é‡‡æ ·åˆ°çš„åŠ¨ä½œéƒ½ä¼šæå‡å…¶å¯¹åº”çš„æ¦‚ç‡ï¼Œä½†è¿™ä¸æ„å‘³ç€æ²¡è¢«é‡‡æ ·åˆ°çš„åŠ¨ä½œå°±ä¸å¥½ã€‚
  
    ![æˆªå±2025-02-22 ä¸‹åˆ3.57.43.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/%E6%88%AA%E5%B1%8F2025-02-22_%E4%B8%8B%E5%8D%883.57.43.png)
    
- é€šè¿‡åŸºçº¿ï¼Œæˆ‘ä»¬å¯ä»¥åªå¯¹$R(\tau)>b$æå‡$\log p_\theta\left(a_t^n \mid s_t^n\right)$ã€‚

<aside>
ğŸ’¡

æ”¹è¿›æ–¹æ³•2ï¼šæ›´ç²¾ç»†åŒ–åœ°æ§åˆ¶æ¯ä¸ªæ›´æ–°çš„æƒé‡ â†’ Actor-Critic

</aside>

$$
\boxed{\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}A_\phi(s_t, a_t) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)}
$$

- ä¸€å±€èµ¢äº†ï¼Œåˆ™æ‰€æœ‰çš„åŠ¨ä½œéƒ½æ˜¯ä¼˜ç§€çš„åŠ¨ä½œå—ï¼Ÿæœªå¿… â†’ æ›´ç²¾ç»†åŒ–åœ°æ§åˆ¶æ¢¯åº¦çš„æƒé‡ã€‚
- Advantage Function: $A_\phi(s_t, a_t)$
    - ä¸€ç§ä¼°è®¡æ–¹æ³•ï¼š$\sum_{t^{\prime}=t}^{T_n} \gamma^{t^{\prime}-t} r_{t^{\prime}}^n-b$ï¼Œä»å½“å‰æ—¶åˆ»tä¹‹åæ‰€èƒ½è·å¾—çš„å¥–åŠ±å‡å»ä¸€ä¸ªåŸºå‡†å€¼ã€‚

æ ¹æ®å¯¹äºæ¢¯åº¦æ›´æ–°å‰é¢ç³»æ•°çš„é€‰æ‹©æœ‰ä¸åŒçš„æ–¹æ³•ï¼Œæ¯”å¦‚**REINFORCE**é€‰æ‹©çš„æ˜¯

$$
\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} G_t^n \nabla \log \pi_\theta\left(a_t^n \mid s_t^n\right)\\
G_t=\sum_{k=t+1}^T \gamma^{k-t-1} r_k
$$

![æˆªå±2025-02-22 ä¸‹åˆ4.14.57.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/%E6%88%AA%E5%B1%8F2025-02-22_%E4%B8%8B%E5%8D%884.14.57.png)

![æˆªå±2025-02-22 ä¸‹åˆ4.17.02.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/%E6%88%AA%E5%B1%8F2025-02-22_%E4%B8%8B%E5%8D%884.17.02.png)

![REINFORCEæ˜¯onlineçš„](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/%E6%88%AA%E5%B1%8F2025-02-22_%E4%B8%8B%E5%8D%884.20.03.png)

REINFORCEæ˜¯onlineçš„

### Critic (Bellman Equation)

**Bellman Equation**

$$
\boxed{V(s)=\underbrace{R(s)}_{\text {å³æ—¶å¥–åŠ± }}+\underbrace{\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {æœªæ¥å¥–åŠ±çš„æŠ˜æ‰£æ€»å’Œ }}}
$$

- $p(s'|s)$: ä»å½“å‰çŠ¶æ€è½¬ç§»åˆ°æŸä¸ªçŠ¶æ€çš„æ¦‚ç‡ï¼ˆåŒ…å«ç­–ç•¥ä»¥åŠç¯å¢ƒè½¬ç§»2éƒ¨åˆ†ï¼‰
- æ¨å¯¼Bellman Equationï¼Œç”¨åˆ°**å…¨æœŸæœ›å…¬å¼**ï¼š$\mathbb{E}[X]=\sum_i \mathbb{E}\left[X \mid A_i\right] p\left(A_i\right)$
  
    ![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%203.png)
    
- ä¾‹å­ï¼šå¦‚æœæœ‰Transitionå¯ä»¥ç›´æ¥æœ‰è§£æè§£
  
    $$
    \left(\begin{array}{c}V\left(s_1\right) \\V\left(s_2\right) \\\vdots \\V\left(s_N\right)\end{array}\right)=\left(\begin{array}{c}R\left(s_1\right) \\R\left(s_2\right) \\\vdots \\R\left(s_N\right)\end{array}\right)+\gamma\left(\begin{array}{cccc}p\left(s_1 \mid s_1\right) & p\left(s_2 \mid s_1\right) & \ldots & p\left(s_N \mid s_1\right) \\p\left(s_1 \mid s_2\right) & p\left(s_2 \mid s_2\right) & \ldots & p\left(s_N \mid s_2\right) \\\vdots & \vdots & \ddots & \vdots \\p\left(s_1 \mid s_N\right) & p\left(s_2 \mid s_N\right) & \ldots & p\left(s_N \mid s_N\right)\end{array}\right)\left(\begin{array}{c}V\left(s_1\right) \\V\left(s_2\right) \\\vdots \\V\left(s_N\right)\end{array}\right)
    $$
    
    $$
    \begin{aligned}\boldsymbol{V} & =\boldsymbol{R}+\gamma \boldsymbol{P} \boldsymbol{V} \\\boldsymbol{I} \boldsymbol{V} & =\boldsymbol{R}+\gamma \boldsymbol{P} \boldsymbol{V} \\(\boldsymbol{I}-\gamma \boldsymbol{P}) \boldsymbol{V} & =\boldsymbol{R} \\\boldsymbol{V} & =(\boldsymbol{I}-\gamma \boldsymbol{P})^{-1} \boldsymbol{R}\end{aligned}
    $$
    
- ä¾‹å­ï¼šå¯ä»¥ç”¨**è’™ç‰¹å¡æ´›æ³•ç›´æ¥ä¼°è®¡æŸä¸ªçŠ¶æ€çš„ä»·å€¼**
  
    ![æˆªå±2025-02-23 ä¸Šåˆ11.11.00.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/%E6%88%AA%E5%B1%8F2025-02-23_%E4%B8%8A%E5%8D%8811.11.00.png)
    
- ä¾‹å­ï¼š**åŠ¨æ€è§„åˆ’** (Bootstrapping + Bellman update)
  
    ![æˆªå±2025-02-23 ä¸Šåˆ11.12.13.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/%E6%88%AA%E5%B1%8F2025-02-23_%E4%B8%8A%E5%8D%8811.12.13.png)
    

**Qå‡½æ•°å’ŒVå‡½æ•°çš„å…³ç³» ï¼ˆQå‡½æ•°çš„Bellman Equationï¼‰**

# PPO

> è®ºæ–‡ï¼š[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)
OpenAI
> 

## é¢„å¤‡çŸ¥è¯†

### Policy Gradient Methods

$$
\boxed{\hat{g}_t=\hat{\mathbb{E}}_t\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) \hat{A}_t\right]}
$$

- $\hat{g}_t$: policy gradient estimator (tæ—¶åˆ»)
- $\pi_\theta$: å¾…ä¼˜åŒ–çš„policy, $\theta$æ˜¯å¯¹åº”ç½‘ç»œçš„å‚æ•°
- $\hat{A}_t$: estimator of advantage function
- $\mathbb{E}_t[\ldots]$: ä¸€ä¸ªbatchçš„å¹³å‡å€¼ï¼ˆempirical averageï¼‰
- è¯æ˜ï¼šä¸ºä½•è¿™ä¸ªæ¢¯åº¦æ–¹å‘æ˜¯æˆ‘ä»¬å¸Œæœ›çš„æ›´æ–°æ–¹å‘ã€‚
  
    ![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%201.png)
    

**å…·ä½“å®è·µçš„æ—¶å€™å¯ä»¥ç›´æ¥ä¼˜åŒ–ä¸‹é¢è¿™ä¸ªæŸå¤±(æˆ‘ä»¬å¸Œæœ›å…¶æœ€å¤§åŒ–)**

$$
\boxed{L^{P G}(\theta)=\hat{\mathbb{E}}_t\left[\log \pi_\theta\left(a_t \mid s_t\right) \hat{A}_t\right]}
$$

- è®ºæ–‡é‡Œè¯´æ˜äº†å¦‚æœç›´æ¥ç”¨åŒä¸€æ‰¹é‡‡æ ·æ•°æ®ï¼ˆtrajectoryï¼‰æ¥å¤šæ¬¡æ›´æ–°å¯èƒ½ä¼šå¯¼è‡´ç ´åæ€§çš„å¾ˆå¤§çš„æ›´æ–°ã€‚
- **å› ä¸ºè¿™ä¸ªå‡½æ•°çš„æ¢¯åº¦å°±æ˜¯ä¸Šå¼çš„$\hat{g}$ï¼Œæ¢è¨€ä¹‹è¿™ä¸ªå‡½æ•°å–åˆ°æå€¼çš„æ¢¯åº¦æ–¹å‘å°±æ˜¯æˆ‘ä»¬ä¸Šé¢å¸Œæœ›ä¼˜åŒ–çš„æ¢¯åº¦æ–¹å‘ã€‚**
  
    Implementations that use automatic differentiation software work by constructing an objective function whose gradient is the policy gradient estimator
    
- å‚è€ƒï¼š[https://zhuanlan.zhihu.com/p/31278940](https://zhuanlan.zhihu.com/p/31278940)

### Importance Sampling

### Importance Sampling

- å› ä¸ºåŠ äº†Importance Samplingæ‰éœ€è¦æ§åˆ¶ä¸¤ä¸ªç­–ç•¥åˆ†å¸ƒä¸èƒ½å·®å¤ªè¿œ

## å…·ä½“å®è·µ

# DPO

> è®ºæ–‡ï¼š[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)
> 

![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%204.png)

- åŸå§‹çš„RLHFé€šå¸¸åŒ…å«3ä¸ªç¯èŠ‚ï¼š
    1. **Supervised fine-tuning (SFT)** â†’ $\pi^{\mathrm{SFT}}$: åœ¨é«˜è´¨é‡çš„æ•°æ®ä¸Šè®­ç»ƒï¼ˆå¯¹è¯ã€æ‘˜è¦ç­‰ï¼‰
    2. **Preference Sampling & Reward Learning**: 
        1. ç»™å®šprompt $x$ï¼Œé‡‡æ ·æ¨¡å‹è¾“å‡ºï¼š$\left(y_1, y_2\right) \sim \pi^{\mathrm{SFT}}(y \mid x)$
        2. äººå·¥æŒ‘é€‰å“ªä¸ªè¾“å‡ºæ›´å¥½ï¼š$y_w \succ y_l \mid x$ï¼Œ$y_w$æ˜¯ä¸¤è€…ä¹‹é—´æ›´å¥½çš„é‚£ä¸ªã€‚
        3. 
    3. **RL optimization**: 
- å¯¹PPOçš„æ”¹è¿›ï¼Œå»æ‰RMè®­ç»ƒå’ŒRLç¯èŠ‚ã€‚åªéœ€è¦åŠ è½½ä¸€ä¸ªæ¨ç†æ¨¡å‹å’Œä¸€ä¸ªè®­ç»ƒæ¨¡å‹ï¼Œç›´æ¥åœ¨åå¥½æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå³å¯ã€‚

## é¢„å¤‡çŸ¥è¯†

### KLæ•£åº¦

> è¡¡é‡2ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå€¼è¶Šå°ï¼Œä¸¤è€…ä¹‹é—´çš„å·®å¼‚è¶Šå°ã€‚
> 

$$
D_{\mathrm{KL}}(P \| Q)=\int P(x) \log \frac{P(x)}{Q(x)} d x
$$

- $P(x)$ï¼šç›®æ ‡åˆ†å¸ƒ
- $Q(x)$ï¼šè¿‘ä¼¼åˆ†å¸ƒï¼ŒKLæ•£åº¦è¡¡é‡çš„æ˜¯

æ€§è´¨ï¼š

1. **éè´Ÿæ€§**ï¼š$D_{\mathrm{KL}}(P \| Q) \geq 0$ï¼Œå½“ä¸”ä»…å½“$P(x)=Q(x)$æ—¶å–åˆ°æœ€å°å€¼0
2. **éå¯¹ç§°æ€§**ï¼š$D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$

- éè´Ÿæ€§è¯æ˜ï¼š
  
    ![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%205.png)
    

### Bradley-Terry æ¨¡å‹

> å¯¹äºèƒœç‡è¿›è¡Œå»ºæ¨¡ï¼Œè®ºæ–‡é‡Œä¹Ÿæåˆ°è¿˜æœ‰å…¶ä»–æ¨¡å‹ (Plackett-Luce ranking models)
> 
- å¯¹è±¡$O_i$çš„å¼ºåº¦ï¼š$\lambda_i$
- $O_i$å¯¹$O_j$çš„èƒœç‡

$$
P(i>j)=\frac{\lambda_i}{\lambda_i+\lambda_j}
$$

- å¯ä»¥ç”¨æå¤§ä¼¼ç„¶æ³•æ¥ä¼°è®¡å¯¹åº”çš„å¼ºåº¦å‚æ•°$\lambda_i$çš„å€¼ï¼š

$$
L(\boldsymbol{\lambda})=\prod_{(i, j) \in \text { pairs }}\left(\frac{\lambda_i}{\lambda_i+\lambda_j}\right)^{x_{i j}}\left(\frac{\lambda_j}{\lambda_i+\lambda_j}\right)^{1-x_{i j}}\\

x_{ij} = 1, å¦‚æœièµ¢\\
x_{ij} = 0, å¦‚æœiè¾“
$$

- $L(\boldsymbol{\lambda})$ï¼šå½“å‰å‚æ•°ä¸‹ï¼Œè§‚å¯Ÿåˆ°æ‰€æœ‰ä¸¤ä¸¤æ¯”èµ›(pairs)çš„èƒœç‡ã€‚æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿé€šè¿‡è°ƒæ•´$\lambda_i$æ¥ä½¿å¾—ä¼°è®¡å¾—åˆ°çš„èƒœç‡æœ€å¤§åŒ–ã€‚

ä¸ªäººæ„Ÿè§‰ï¼š$\lambda_i \geq 0$ï¼Œå¦åˆ™èƒœç‡æœ‰å¯èƒ½ä¼šè¶…è¿‡1ã€‚

### DPO

1. **Reward Model**ä¼˜åŒ–ç›®æ ‡æ¨å¯¼

$$
\boxed{P\left(y_w \succ y_l \mid x\right)=\sigma\left(r\left(x, y_w\right)-r\left(x, y_l\right)\right) \quad\quad \text{(1)}}
$$

- $y_w$: æ¨¡å‹è¾“å‡ºçš„**æ›´ä¼˜**çš„ç­”æ¡ˆ
$y_l$: æ¨¡å‹è¾“å‡ºçš„**æ¬¡ä¼˜**çš„ç­”æ¡ˆ
- $\sigma(x)$: sigmoidå‡½æ•°ï¼Œ$\sigma(x)=\frac{1}{1+e^{-x}}$
- æ¨å¯¼è¿‡ç¨‹ï¼šç”¨åˆ°2ä¸ªå‡è®¾(Bradley-Terryæ¨¡å‹ & $\lambda_i = e^{r(x, y_i)}$)
  
    $$
    \begin{aligned}
    P\left(y_w \succ y_l \mid x\right)&=\frac{e^{r\left(x, y_w\right)}}{e^{r\left(x, y_w\right)}+e^{r\left(x, y_l\right)}}\\
    &=\frac{1}{1+e^{r\left(x, y_l\right)-r\left(x, y_w\right)}}\\
    &=\sigma\left(r\left(x, y_w\right)-r\left(x, y_l\right)\right)
    \end{aligned}
    $$
    

ç”±æ­¤ï¼Œå¯¹äºä¸€ä¸ªåå¥½æ•°æ®é›†

$$
\mathcal{D}=\left\{x^{(i)}, y_\omega^{(i)}, y_l^{(i)}\right\}_{i=1}^N
$$

æˆ‘ä»¬å¯ä»¥æ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡æ„å»ºä¸€ä¸ªreward model: $r_\phi(x, y)$ï¼Œå…¶ç›®æ ‡æ˜¯ä½¿å¾—åœ¨è¿™ä¸ªæ•°æ®ä¸Šåå¥½æ¦‚ç‡çš„ä¹˜ç§¯æœ€å¤§ï¼ˆå‚è€ƒBTæ¨¡å‹ä¸€èŠ‚ï¼‰ã€‚

$$
\mathcal{L}(r_\phi, \mathcal{D})=\prod_{\left(x, y_w, y_l\right) \sim \mathcal{D}} \sigma\left(r_{\phi} \left(x, y_w\right)-r_{\phi}\left(x, y_l\right)\right)
$$

å¦‚æœæˆ‘ä»¬è¦å¾—åˆ°ä¸€ä¸ªæœ€å°åŒ–çš„lossï¼Œå¯ä»¥å†™ä½œï¼š

$$
\boxed{\mathcal{L}_{\mathcal{R}}\left(r_\phi, \mathcal{D}\right)=-\mathbb{E}_{\left(x, y_\omega, y_l\right) \sim \mathcal{D}} \log \sigma\left(r_\phi\left(x, y_w\right)-r_\phi\left(x, y_l\right)\right)\quad\quad \text{(2)}}
$$

- ä¸€èˆ¬reward model $r_\phi(x, y)$ä»SFTç»“æŸåçš„æ¨¡å‹$\pi^{\mathrm{SFT}}(y \mid x)$åˆå§‹åŒ–ï¼Œå¹¶åœ¨æœ€åçš„transformer layerä¸Šå¢åŠ ä¸€å±‚linearå±‚æ¥å¾—åˆ°ä¸€ä¸ªrewardçš„å€¼ã€‚

1. DPOæŸå¤±å‡½æ•°æ¨å¯¼

å…ˆä»ä¹‹å‰çš„åšæ³•å¼€å§‹ï¼Œ å‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªreward model $r_\phi(x,y)$ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å¦‚ä¸‹çš„æŸå¤±æ¥ä¼˜åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆPolicy modelï¼‰$\pi_\theta(y \mid x)$ã€‚

$$
\boxed{\max _{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y \mid x)}\left[r_\phi(x, y)\right]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi_\theta(y \mid x) \| \pi_{\mathrm{ref}}(y \mid x)\right] \quad\quad\text{(3)}}
$$

- $\pi_\theta(y \mid x)$ï¼šå½“å‰å¾…ä¼˜åŒ–æ¨¡å‹(llm)ï¼Œä¹Ÿè¢«ç§°ä¸ºPolicy Model
- $\pi_{\mathrm{ref}}(y \mid x)$ï¼šå‚è€ƒçš„ç­–ç•¥ï¼Œä¸€èˆ¬å°±æ˜¯SFTä¹‹åçš„æ¨¡å‹ $\pi^{\mathrm{SFT}}$
- $\beta$ï¼šè¶…å‚æ•°ï¼Œæˆ‘ä»¬å¸Œæœ›è®­ç»ƒå®Œçš„æ¨¡å‹å’Œè®­ç»ƒä¹‹å‰æ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒæ¯”è¾ƒæ¥è¿‘ã€‚
    - è®ºæ–‡é‡Œè§£é‡Šï¼šé˜²æ­¢æ¨¡å‹åç¦»å¥–åŠ±æ¨¡å‹å‡†ç¡®çš„åˆ†å¸ƒå¤ªè¿œï¼Œä»¥åŠä¿æŒç”Ÿæˆå¤šæ ·æ€§å’Œé˜²æ­¢æ¨¡å¼å´©æºƒä¸ºå•ä¸ªé«˜å¥–åŠ±ç­”æ¡ˆã€‚
      
        The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. 
        
    - å› ä¸ºä¹‹å‰çš„$r_\phi(x,y)$æ˜¯æ ¹æ®$\pi_{\mathrm{ref}}(y \mid x)$ï¼ˆå³$\pi^{\mathrm{SFT}}$ï¼‰è®­ç»ƒå¾—åˆ°çš„ï¼Œå¦‚æœæ¢ä¸ªpolicyï¼Œå¯¹åº”çš„å¥–åŠ±å‡½æ•°å¯èƒ½å·®è·æ¯”è¾ƒå¤§ã€‚æ‰€ä»¥ä¸ºäº†èƒ½å¤ç”¨è¿™ä¸ªå¥–åŠ±å‡½æ•°ï¼Œå¾—åŠ ä¸Šè¿™ä¸ªKLé™åˆ¶ã€‚
- **ä¹‹å‰å¸¸è§åšæ³•**ï¼š
    - $r(x, y)=r_\phi(x, y)-\beta\left(\log \pi_\theta(y \mid x)-\log \pi_{\mathrm{ref}}(y \mid x)\right)$
    - ç”¨PPOæœ€å¤§åŒ–(3)å¼ï¼Œä»è€Œå¾—åˆ°æœ€åçš„policy model $\pi_\theta$

---

é‚£ä¹ˆDPOæ˜¯æ€ä¹ˆå¹²æ‰reward model $r_\phi(x,y)$çš„å‘¢ï¼Ÿåˆ†2æ­¥

**ç¬¬ä¸€æ­¥ï¼šè¯æ˜ï¼ˆ3ï¼‰å¼è¦æ±‚çš„æœ€ä¼˜ç­–ç•¥æ˜¯æœ‰æ˜¾å¼è§£çš„**ã€‚

$$
\boxed{\pi(y \mid x)=\pi^*(y \mid x)=\frac{1}{Z(x)} \pi_{r e f}(y \mid x) e^{\frac{1}{\beta} r_\phi(x, y)} \quad\quad\text{(4)}}
$$

$$
Z(x)=\sum_y \pi_{r e f}(y \mid x) e^{\frac{1}{\beta} r_\phi(x, y)}
$$

- è¯æ˜ï¼šæ ¸å¿ƒæ˜¯å‡‘å‡ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œå†åˆ©ç”¨KLæ•£åº¦éè´Ÿæ€§å¾—åˆ°æœ€ä¼˜çš„ç­–ç•¥ã€‚
  
    ![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%206.png)
    

**ç¬¬äºŒæ­¥ï¼šç›´æ¥è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„èƒœç‡ï¼Œå›å½’åˆ°BTæ¨¡å‹ï¼Œä»è€Œå¾—åˆ°DPOæŸå¤±ã€‚**

$$
\boxed{\mathcal{L}_{D P O}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right]}
$$

- $\pi_\theta(y \mid x)$: å½“å‰å¾…ä¼˜åŒ–çš„æ¨¡å‹ã€‚

- è¯æ˜ï¼šæ ¸å¿ƒæ˜¯æ—¢ç„¶rewardå’Œpolicyå­˜åœ¨ä¸€å®šçš„å…³ç³»ï¼Œä¸è®­ç»ƒä¸€ä¸ªreward modelä¸å¦‚ç›´æ¥è®­ç»ƒpolicy model $\pi_\theta(y|x)$
  
    ![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%207.png)
    

### DPO æ¢¯åº¦çš„è§£é‡Š

![æˆªå±2025-02-16 ä¸‹åˆ2.54.16.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/%E6%88%AA%E5%B1%8F2025-02-16_%E4%B8%8B%E5%8D%882.54.16.png)

$$
\hat{r}_\theta(x, y)=\beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}
$$

- ç›´è§‚ç†è§£æ˜¯ï¼šæ›´æ–°ä¼šå¢å¤§preferedç­”æ¡ˆçš„æ¦‚ç‡ï¼Œå‡å°ä¸å–œæ¬¢çš„ç­”æ¡ˆçš„æ¦‚ç‡ï¼Œä»¥åŠå½“rewardä¼°è®¡æœ‰è¯¯çš„æ—¶å€™ä¼šæœ‰è¾ƒå¤§æ›´æ–°ã€‚
- æ¢¯åº¦çš„æ¨å¯¼ï¼š
  
    ![image.png](RLHF%20180e56f56d3c80ce9fbec0232dc4f1e8/image%208.png)
    

### DPO å®ç°æ­¥éª¤

Case1: å¦‚æœä»0å¼€å§‹ï¼ˆæ²¡æœ‰å¯¹åº”çš„åå¥½æ•°æ®ï¼‰

1. $y_1, y_2 \sim \pi_{\mathrm{ref}}(\cdot \mid x)$ é‡‡æ ·ï¼Œäººå·¥æ ‡æ³¨å¾—åˆ°åå¥½æ•°æ®$\mathcal{D}=\left\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\right\}_{i=1}^N$ã€‚æ­¤æ—¶$\pi_{ref}$å°±æ˜¯å¾…ä¼˜åŒ–çš„æ¨¡å‹ã€‚
2. ä¼˜åŒ–DPOæŸå¤±ã€‚

Case2: **å¤ç”¨å¼€æºçš„DPOæ•°æ®é›†**

1. **å¦‚æœå¼€æºçš„æ•°æ®é›†æœ‰å…¬å¼€é‡‡æ ·çš„æ¨¡å‹æƒé‡ï¼Œåˆ™ç”¨å¯¹åº”çš„æ¨¡å‹æƒé‡åˆå§‹åŒ–**$\pi_{\mathrm{ref}}=\pi^{\mathrm{SFT}}$
2. é€šè¿‡è®­ç»ƒå¾…ä¼˜åŒ–çš„æ¨¡å‹æ¥é€¼è¿‘é‡‡æ ·ç”¨çš„æ¨¡å‹$\pi_{\mathrm{ref}}$
   
    $$
    \pi_{\mathrm{ref}}=\arg \max _\pi \mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi\left(y_w \mid x\right)\right]
    $$
    
3. åˆ©ç”¨$\pi_{ref}$è¿›è¡ŒDPOè®­ç»ƒã€‚

## å…·ä½“å®è·µ

- æ•°æ®æ ¼å¼ï¼š
    - prompt: ä¸Šä¸‹æ–‡è¾“å…¥
    - chosen: æ›´å¥½çš„å›ç­”
    - rejected: æ›´å·®çš„å›ç­”
- ä¾‹å­ï¼š[jingyaogong/minimind_dataset](https://huggingface.co/datasets/jingyaogong/minimind_dataset?row=33)
  
    ```python
    {
    	"chosen": [
    	{"content": "How many moles of HBr are required to react with 2 moles of C2H6 to form 2 moles of C2H5Br along with 2 moles of H2?", "role": "user"}, 
    	{"content": "To determine the number of moles of HBr required to react with 2 moles of C2H6 (ethane) to form 2 moles of C2H5Br (ethyl bromide) and 2 moles of H2, we need to write the balanced chemical equation for the reaction.\n\nThe reaction is a halogenation reaction, specifically the reaction of ethane with hydrogen bromide (HBr) to form ethyl bromide and hydrogen gas. The balanced chemical equation for this reaction is:\n\n\\[ \\text{C}_2\\text{H}_6 + \\text{HBr} \\rightarrow \\text{C}_2\\text{H}_5\\text{Br} + \\text{H}_2 \\]\n\nHowever, this equation is not balanced as written. To balance it, we need to ensure that the number of atoms of each element on the reactant side is equal to the number of atoms of that element on the product side. The balanced equation is:\n\n\\[ \\text{C}_2\\text{H}_6 + \\text{HBr} \\rightarrow \\text{C}_2\\text{H}_5\\text{Br} + \\text{H}_2 \\]\n\nUpon closer inspection, we see that the equation is already balanced as written, with 1 mole of C2H6 reacting with 1 mole of HBr to produce 1 mole of C2H5Br and 1 mole of H2.\n\nGiven that you have 2 moles of C2H6, and the stoichiometry of the reaction is 1:1 for C2H6 to HBr, you would need 2 moles of HBr to react with 2 moles of C2H6 to form 2 moles of C2H5Br and 2 moles of H2.\n\nTherefore, the number of moles of HBr required is **2 moles**.", "role": "assistant"}], 
    	"rejected": [
    	{"content": "How many moles of HBr are required to react with 2 moles of C2H6 to form 2 moles of C2H5Br along with 2 moles of H2?", "role": "user"}, 
    	{"content": "To answer this question, we need to write down the chemical equation representing the reaction between hydrogen bromide (HBr) and ethane (C2H6):\n\nC2H6(g) + 2HBr(g) \u2192 C2H5Br(l) + H2(g)\n\nNow, we can set up a mole ratio based on the coefficients in front of each substance in the balanced equation:\n\nMoles of HBr : Moles of C2H6 :: 2 : 1\n\nWe also know that there are two moles of C2H6 present in our starting mixture, so we can substitute this value into the above mole ratio to determine the amount of HBr needed:\n\n(2 mol C2H6)/(1 mol C2H6) x 2 mol HBr = 4 mol HBr\n\nSo, four moles of HBr would be required to completely react with two moles of C2H6 according to the given stoichiometric relationship.", "role": "assistant"}]}
    ```
    
- è®­ç»ƒä»£ç ï¼šæ¥è‡ªè®ºæ–‡
  
    ```python
    import torch.nn.functional as F 
    
    def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta): 
    	""" 
    	pi_logps: policy logprobs, shape (B,) 
    	ref_logps: reference model logprobs, shape (B,) 
    	yw_idxs: preferred completion indices in [0, B-1], shape (T,) 
    	yl_idxs: dispreferred completion indices in [0, B-1], shape (T,) 
    	beta: temperature controlling strength of KL penalty 
    	
    	Each pair of (yw_idxs[i], yl_idxs[i]) represents the indices of a single preference pair. 
    	""" 
    	pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs] 
    	ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs] 
    	
    	pi_logratios = pi_yw_logps - pi_yl_logps 
    	ref_logratios = ref_yw_logps - ref_yl_logps 
    	losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios)) 
    	rewards = beta * (pi_logps - ref_logps).detach() 
    	
    	return losses, rewards
    ```
    
    - $\beta = 0.1, BS=65, RMSProp, lr=1e-6$
- è®­ç»ƒä»£ç ï¼štrlåº“
  
    ```python
    
    ```
    
- è®­ç»ƒä»£ç ï¼šæ‰‹æ“

## å‚è€ƒ

- [https://blog.csdn.net/qq_36803941/article/details/142251643](https://blog.csdn.net/qq_36803941/article/details/142251643)
- [https://zhuanlan.zhihu.com/p/644911957](https://zhuanlan.zhihu.com/p/644911957)

# GRPO

> è®ºæ–‡ï¼š[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)